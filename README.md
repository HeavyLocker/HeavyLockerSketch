# HeavyLocker: Lock Heavy Hitters in Multiple Data Streams
This repository contains all the related code of our paper "HeavyLocker: Lock Heavy Hitters in Multiple Data Streams".

## Introduction
In recent years, sketching has emerged as a pivotal technique for identifying heavy hitters (items with high frequency) in large-scale data streams. Despite this progress, the majority of existing sketch algorithms are tailored primarily for detecting local heavy hitters within a single data stream, with only a select few capable of extending their application to global heavy hitters across multiple data streams. A common challenge encountered by these algorithms is balancing performance with accuracy. To address this challenge, we introduce HeavyLocker, a novel sketch algorithm that takes advantage of a distinct feature of real data streams: the uniformity of heavy hitters. By leveraging this attribute, HeavyLocker precisely locks and protects potential heavy hitters during the data stream processing, ensuring accuracy in local heavy hitter detection without compromising on speed. This unique capability also facilitates its application to global detection tasks. Through theoretical analysis, we validate the efficacy of HeavyLocker's locking mechanism. Our extensive experiments show that HeavyLocker outperforms five benchmarked algorithms in accuracy and maintains fast speed for both local and global heavy hitter detection, significantly reducing errors by up to an order of magnitude compared to the renowned Double-Anonymous Sketch.

## About this repository
We implement our HeavyLocker in C++ and compare our results with CM sketch+heap, Elastic sketch, MV Sketch, Unbiased SpaceSaving, and Double-anonymous Sketch. It is notable that we implement CMSketch with a stream summary data structure to make it invertible.

### Files
We implemented the methods separately in goodMSketch.h, CMSketch.h, ElasticSketch.h, Mvsketch.h, USS.h and DASketch.h. Other files in this repository are listed below:
-BaseSketch.h: Interface file of functions, which defines the basic operations of the framework such as insert, merge, and query.
-BOBHASH32.h: File that defines the Hash function
-BOBHASH64.h: File that defines the Hash function
-ssummary.h: Realize the stream summary data structure to make methods invertible
-LossyStrategy.h: Realize four different replace methods for HeavyLocker
-param.h: Configuration file for the parameters
-0.dat: Dataset
-local_zipf.sh,node_num.sh,global_zipf_merge.sh: Script files for experiments

## Reproduce experiments

### Dataset
We use two kinds of datasets in experiments: CAIDA (http://www.caida.org/data/overview/) and Zipf, which is a series of synthetic traces generated by Web Polygraph. You can generate it based on the Zipf distribution. In our experiment, the skewness of the traces ranges from 0.6 to 2.1, and each trace
contains 32.0 million items in total. We also provide a small dataset called 0.dat in this folder for you to test our framework. You can find the CAIDA dataset in "release" on the right side of the page.

### Compile and run
You can build the program with the following commands:
$ cd HeavyLocker
$ make
After compiling successfully, you can experiment by running the executable file, whose name is defined by Makefile. Note that you can change the configuration of the experiment by changing the parameters. For example,you can execute the following statement:
$ ./merge_add -d ../../real_data/1.dat -m 30 -n 4 -s 0 -t 0.0001 -e 6 -l 2.8 -h 1

The parameters and their meanings are as follows:
param   type     meaning
-d      string   Dataset
-o      string   Output file
-m      int      Memory（KB）
-n      int      Node_num
-s      bool     Whether the same flow can be present on different nodes
-t      float    Threshold of HH 
-e      int      Depth of HeavyLocker
-l      float    Lock_threshold of HeavyLocker
-h      int      Num of hash functions 


